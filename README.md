# 114-1-rl-mountain-car
This repository documents a comprehensive Deep Reinforcement Learning (DRL) study conducted on the classic Gymnasium environment: MountainCar-v0. We train and compare three major DRL algorithms: DQN, PPO, and A2C to analyze their performance, convergence speed, and stability under sparse and delayed reward conditions.

# Plan

- **Step 1: Environment Selection**
    - Mountain Car
- **Step 2: Environment Analysis**
    - Identify the reward structure
- **Step 3: Algorithm Selection**
    - Consider using 2-3 different algorithms for comparison (e.g., DQN, PPO, A2C)
    - Justify algorithm choices
- **Step 4: Training Setup**
    - Configure hyperparameters (learning rate, batch size, epsilon decay, etc.)
    - Set up TensorBoard for tracking metrics
    - Define training duration (number of episodes/epochs)
    - Implement reward shaping if needed
- **Step 5: Training and Experimentation**
    - Train baseline model
    - Experiment with different architectures or hyperparameters
    - Document all experiments and results
    - Save best performing models
- **Step 6: Results Collection**
    - Generate training curves (reward, loss, episode length)
    - Record final performance metrics
    - Create visualizations of agent behavior
    - Compare results across different algorithms/configurations
- **Step 7: Analysis and Documentation**
    - Write down interesting observations using pen and paper:
        - Challenges encountered during training
        - Insights about the environment dynamics
        - Why certain approaches worked or failed
        - Comparison with 2048 training experience
    - Take clear photo/scan of handwritten notes
    - Organize all results and findings in the report
